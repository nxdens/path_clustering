{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import re\n",
    "import nibabel as nib\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, shutil\n",
    "import pickle as cPickle\n",
    "import ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLabels(folderName, labels,originalPaths):\n",
    "    if os.path.exists(folderName):\n",
    "        shutil.rmtree(folderName)\n",
    "    os.mkdir(folderName)\n",
    "    fileNamesDict = {}\n",
    "    print(labels)\n",
    "    for lineNum, label in enumerate(labels):\n",
    "        fileName = \"\".join((folderName,\"/\",str(label),\".txt\"))\n",
    "        if fileName in fileNamesDict:\n",
    "            file = fileNamesDict[fileName]\n",
    "        else:\n",
    "            file =  open(fileName, \"w+\")\n",
    "            fileNamesDict[fileName] = file\n",
    "        originalPaths[lineNum].astype(np.int)\n",
    "        line = str(originalPaths[lineNum])[1:-1] #flattened paths contain the unedited paths hashed by the root node\n",
    "        line = re.sub(\"'|\\n|[|]\", \"\", line) #get rid of junk characters when converting from an array to a string\n",
    "        file.write(\" \".join((line,\"\\n\")))\n",
    "    allLabelFiles = open(\"\".join((\"all\",folderName,\"Files.txt\")),\"w+\")\n",
    "    allLabelFiles.write(\"bv_loadDataImage      SWI_Images_resize10.nii \\n\")\n",
    "    allLabelFiles.write(\"bv_loadSegmentedImage csf_seg_8bit_resize10_8bit.nii \\n\")\n",
    "    for fileKeys in fileNamesDict:\n",
    "        allLabelFiles.write(\"    \".join((\"bv_readPathsFromFile\",fileNamesDict[fileKeys].name)))\n",
    "        allLabelFiles.write(\"\\n\")\n",
    "        fileNamesDict[fileKeys].close()\n",
    "    #allPathFiles.write(str(clusteringMethod))\n",
    "    allLabelFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipycache extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipycache\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = 0 #toggle for print statements\n",
    "EXPLORE = 0 #toggle for ploting clusters and seeing number of components with pca\n",
    "USING_ROOT_BUNCHING = 0 #toggle between modes of operation\n",
    "ZERO_PADDED = 0 #set to 1 to pad the intensities with 0s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": [
     "data_loading"
    ]
   },
   "outputs": [],
   "source": [
    "paths = len(open(\"pathFile1.txt\",\"r\").readlines()) if DEBUG else None\n",
    "longestPathLength = int((len(max(open(\"pathFile1.txt\",\"r\"),key=len).split(\" \"))-1)/3) if DEBUG else None\n",
    "print(paths) if DEBUG == 1 else None\n",
    "print(longestPathLength) if DEBUG else None\n",
    "dilationRadius = 0 #has to be greater than or equal to 0\n",
    "numIntensityParams = ((2*(dilationRadius+1)-1)**2) #number of intensities to use\n",
    "if dilationRadius <0 :\n",
    "    numIntensityParams = 0\n",
    "print(numIntensityParams) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": [
     "data_preprocess",
     "data_loading"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skipped the cell's code and loaded variables swiData, swiImage from file 'C:\\Users\\Linghai Wang\\Documents\\GPN\\Clustering\\imageData.pkl'.]\n"
     ]
    }
   ],
   "source": [
    "%%cache imageData.pkl swiImage swiData\n",
    "swiImage = nib.load(\"SWI_Images_resize10.nii\")\n",
    "swiImage.shape\n",
    "swiData = swiImage.get_fdata()\n",
    "print(swiData.shape) if DEBUG else None\n",
    "\n",
    "\n",
    "#swiData = swiData/np.max(np.abs(swiData),axis=0) #normalize by the max across entire image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622.0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(swiData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved variables 'allIntensityPaths, allOriginalPaths, allPaths' to file 'C:\\Users\\Linghai Wang\\Documents\\GPN\\Clustering\\loadedAllPaths'.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1575it [00:00, 2185.01it/s]\n"
     ]
    }
   ],
   "source": [
    "%%cache loadedAllPaths allPaths allOriginalPaths allIntensityPaths\n",
    "if not USING_ROOT_BUNCHING:\n",
    "    pathFile = open(\"pathFile1.txt\",\"r\")\n",
    "    allPaths = []\n",
    "    allOriginalPaths = []\n",
    "    allIntensityPaths = []\n",
    "    for lineNum, line in tqdm(enumerate(pathFile)):\n",
    "        currentSplitLine = line.split(\" \")\n",
    "        currentSplitLine = currentSplitLine[:-1]\n",
    "        flatLine = currentSplitLine\n",
    "        allOriginalPaths.append(flatLine)\n",
    "\n",
    "        currentSplitLine = list(map(int,currentSplitLine))\n",
    "\n",
    "        gFoldedLine = np.reshape(currentSplitLine,(int(len(currentSplitLine)/3),3))\n",
    "\n",
    "        paddedFoldedLine = np.pad(gFoldedLine,[(0,0),(0,numIntensityParams)], \"constant\", constant_values = 0)\n",
    "        paddedFoldedLine = np.delete(paddedFoldedLine, 0, 1)#the x value is useless since its just the index of the array so its just noise\n",
    "        intensityLine = np.zeros((paddedFoldedLine.shape[0],numIntensityParams))\n",
    "        if dilationRadius>=0:\n",
    "            for pixelNum, pixel in enumerate(gFoldedLine):\n",
    "                for i in range(numIntensityParams):\n",
    "                    x = int(pixel[0])\n",
    "                    y = int(pixel[1])-dilationRadius+int((i%(2*dilationRadius-1)))\n",
    "                    z = int(pixel[2])-dilationRadius+int(i/(2*dilationRadius-1))\n",
    "\n",
    "                    if x<0 or y<0 or z<0 or x>=swiData.shape[0] or y >= swiData.shape[1] or z >= swiData.shape[2]:\n",
    "                        if ZERO_PADDED:\n",
    "                            pixelIntensity = 0 #622 is max intensity #essentially pad with 0s if the shell goes out of bounds\n",
    "                        else:\n",
    "                            if x<0:\n",
    "                                x = 0\n",
    "                            if y<0:\n",
    "                                y = 0\n",
    "                            if z<0:\n",
    "                                z = 0\n",
    "                            if x>=swiData.shape[0]:\n",
    "                                x = swiData.shape[0]\n",
    "                            if y>=swiData.shape[1]:\n",
    "                                y = swiData.shape[1]\n",
    "                            if z>=swiData.shape[2]:\n",
    "                                z = swiData.shape[2]\n",
    "                    else:\n",
    "                        pixelIntensity = swiData[x][y][z]\n",
    "                    paddedFoldedLine[pixelNum][2+i] = pixelIntensity\n",
    "                    intensityLine[pixelNum][i] = pixelIntensity                    \n",
    "        allPaths.append(paddedFoldedLine)\n",
    "        allIntensityPaths.append(intensityLine)\n",
    "    pathFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true,
    "tags": [
     "data_preprocess"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved variables 'bunchedIntensities, bunchedPaths, flatennedPaths' to file 'C:\\Users\\Linghai Wang\\Documents\\GPN\\Clustering\\loadedBunchPaths.pkl'.]\n"
     ]
    }
   ],
   "source": [
    "%%cache loadedBunchPaths.pkl bunchedPaths flatennedPaths bunchedIntensities\n",
    "if USING_ROOT_BUNCHING:\n",
    "    pathFile = open(\"pathFile1.txt\",\"r\")\n",
    "    # start with just the xyz and intensity of the paths nothing around it yet\n",
    "    bunchedPaths = {}#dictionary with the keys as the root node as a joined spring and the values as a list of all paths with that root node\n",
    "    flatennedPaths = {}#used for initial clustering without intesities \n",
    "    bunchedIntensities = {} # no x,y,z just intensities\n",
    "    for lineNum, line in tqdm(enumerate(pathFile)):\n",
    "        currentSplitLine = line.split(\" \")\n",
    "        currentSplitLine = currentSplitLine[:-1]\n",
    "        flatLine = currentSplitLine\n",
    "        currentSplitLine = list(map(float,currentSplitLine))\n",
    "        gFoldedLine = np.reshape(currentSplitLine,(int(len(currentSplitLine)/3),3))\n",
    "\n",
    "\n",
    "        rootNode = \" \".join(list(map(str,map(int,gFoldedLine[58]))))\n",
    "        gFoldedLine.astype(np.float)\n",
    "        groupArray = []\n",
    "        flatGroupArray = []\n",
    "        intensityArray = []\n",
    "        if rootNode in bunchedPaths: # if the root hasnt been added to the dictionary add it to the dictionary with a list of paths\n",
    "            groupArray = bunchedPaths[rootNode] #join the string to hash a string instead of a tuple since python likes that better\n",
    "            flatGroupArray = flatennedPaths[rootNode]\n",
    "            intensityArray = bunchedIntensities[rootNode]\n",
    "        else:\n",
    "            bunchedPaths[rootNode] = groupArray\n",
    "            flatennedPaths[rootNode] =flatGroupArray \n",
    "            bunchedIntensities[rootNode] = intensityArray\n",
    "\n",
    "        flatGroupArray.append(flatLine)\n",
    "\n",
    "\n",
    "        #gFoldedLine =gFoldedLine/np.max(gFoldedLine,axis=0) #currently normalizaing along the path to capture the variance within a single path but not across each root or the entire image\n",
    "        paddedFoldedLine = np.pad(gFoldedLine,[(0,0),(0,numIntensityParams)], \"constant\", constant_values = 0)\n",
    "        paddedFoldedLine = np.delete(paddedFoldedLine, 0, 1)#the x value is useless since its just the index of the array so its just noise\n",
    "        intensityLine = np.zeros((paddedFoldedLine.shape[0],numIntensityParams))\n",
    "        if dilationRadius>=0:\n",
    "            for pixelNum, pixel in enumerate(gFoldedLine):\n",
    "                for i in range(numIntensityParams):\n",
    "                    x = int(pixel[0])\n",
    "                    y = int(pixel[1])-dilationRadius+int((i%(2*dilationRadius-1)))\n",
    "                    z = int(pixel[2])-dilationRadius+int(i/(2*dilationRadius-1))\n",
    "                    \n",
    "                    if x<0 or y<0 or z<0 or x>=swiData.shape[0] or y >= swiData.shape[1] or z >= swiData.shape[2]:\n",
    "                        \n",
    "                        pixelIntensity = 0 #essentially pad with 0s if the shell goes out of bounds\n",
    "                    else:\n",
    "                        pixelIntensity = swiData[x][y][z]\n",
    "                    paddedFoldedLine[pixelNum][2+i] = pixelIntensity\n",
    "                    intensityLine[pixelNum][i] = pixelIntensity\n",
    "\n",
    "        paddedFoldedLine.astype(np.float)\n",
    "        groupArray.append(paddedFoldedLine)\n",
    "        intensityArray.append(intensityLine)\n",
    "        print(paddedFoldedLine.shape) if DEBUG else None\n",
    "    pathFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true,
    "tags": [
     "data_display"
    ]
   },
   "outputs": [],
   "source": [
    "print(bunchedIntensities.keys()) if DEBUG else None\n",
    "#21 unique roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flatennedPaths.keys()) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    #just to make sure\n",
    "    for keys in bunchedPaths:\n",
    "        bunchedPaths[keys] = np.array(bunchedPaths[keys])#.astype(np.float)\n",
    "        print(bunchedPaths[keys].shape)if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    #just to make sure\n",
    "    for keys in bunchedIntensities:\n",
    "        bunchedIntensities[keys] = np.array(bunchedIntensities[keys])#.astype(np.float)\n",
    "        print(bunchedIntensities[keys].shape) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true,
    "tags": [
     "data_display",
     "data_preprocess"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    for keys in flatennedPaths:\n",
    "        flatennedPaths[keys] = np.array(flatennedPaths[keys])#.astype(np.float)\n",
    "        print(flatennedPaths[keys]) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true,
    "tags": [
     "clustering"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    clusteringMethod = DBSCAN(eps = 3, min_samples = 2) #DBSCAN(eps=0.5, min_samples=5, metric=’euclidean’, metric_params=None, algorithm=’auto’, leaf_size=30, p=None, n_jobs=None)[source]¶\n",
    "    labels = {}\n",
    "    for keys in flatennedPaths:\n",
    "        clusteringMethod.fit(flatennedPaths[keys])\n",
    "        labels[keys] = clusteringMethod.labels_\n",
    "        #print(flatennedPaths[keys].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": [
     "data_display"
    ]
   },
   "outputs": [],
   "source": [
    "print(labels) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPLORE:\n",
    "    #this cell gets the variance ratios for all possible numbers of components for each root node\n",
    "    varianceDictionary = {}\n",
    "    for keys in bunchedPaths:\n",
    "        currentBunch = bunchedPaths[keys]\n",
    "        currentBunch = np.reshape(currentBunch, (currentBunch.shape[0], currentBunch.shape[1]*currentBunch.shape[2])) #flatten\n",
    "        currentBunch = normalize(currentBunch,norm = 'l2', axis = 0) #normalize by root node by a squared average\n",
    "        varianceList = []\n",
    "        for i in range(min(currentBunch.shape[0],currentBunch.shape[1])):\n",
    "            pca = PCA(n_components = i)\n",
    "            pca.fit(currentBunch)\n",
    "            varianceList.append(np.sum(pca.explained_variance_ratio_))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:    \n",
    "    intensityLabels = {} #intensity+x,y,z\n",
    "    for keys in tqdm(bunchedPaths):\n",
    "        clusteringMethod = DBSCAN(eps = .1, min_samples = 2)\n",
    "        currentBunch = bunchedPaths[keys]\n",
    "        currentBunch = np.reshape(currentBunch, (currentBunch.shape[0], currentBunch.shape[1]*currentBunch.shape[2])) #flatten\n",
    "        currentBunch = normalize(currentBunch,norm = 'l2', axis = 0) #normalize by root node by a squared average\n",
    "        pca = PCA(.99)\n",
    "        pca.fit(currentBunch)\n",
    "        transformed = pca.transform(currentBunch)\n",
    "        print(currentBunch.shape) if DEBUG else None\n",
    "        clusteringMethod.fit(transformed)\n",
    "        intensityLabels[keys] = clusteringMethod.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    if dilationRadius > 0:\n",
    "        intensityClusters = {} #just intensity\n",
    "        for keys in tqdm(bunchedIntensities):\n",
    "            clusteringMethod = DBSCAN(eps = .1, min_samples = 2)\n",
    "            currentBunch = bunchedIntensities[keys]\n",
    "            if numIntensityParams >1:\n",
    "                currentBunch = np.reshape(currentBunch, (currentBunch.shape[0], currentBunch.shape[1]*currentBunch.shape[2])) #flatten\n",
    "            currentBunch = normalize(currentBunch,norm = 'l2', axis = 0) #normalize by root node by a squared average\n",
    "            pca = PCA(.99)\n",
    "            pca.fit(currentBunch)\n",
    "            transformed = pca.transform(currentBunch)\n",
    "            print(currentBunch.shape) if DEBUG else None\n",
    "            clusteringMethod.fit(transformed)\n",
    "            intensityClusters[keys] = clusteringMethod.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true,
    "tags": [
     "clustering"
    ]
   },
   "outputs": [],
   "source": [
    "#this cell varys the eps to see the change in the number of clusters with pca set to .95 variance ratio\n",
    "if EXPLORE:\n",
    "    clusterDict = {}\n",
    "    for keys in bunchedPaths:\n",
    "        clusterList = []\n",
    "        for i in tqdm(range(10)):\n",
    "            clusteringMethod = DBSCAN(eps = (i+1)/10, min_samples = 2) #DBSCAN(eps=0.5, min_samples=5, metric=’euclidean’, metric_params=None, algorithm=’auto’, leaf_size=30, p=None, n_jobs=None)[source]¶\n",
    "            currentBunch = bunchedPaths[keys]\n",
    "            currentBunch = np.reshape(currentBunch, (currentBunch.shape[0], currentBunch.shape[1]*currentBunch.shape[2])) #flatten\n",
    "            currentBunch = normalize(currentBunch,norm = 'l2', axis = 0) #normalize by root node by a squared average\n",
    "            for j in range(1,min(currentBunch.shape[0],currentBunch.shape[1])):\n",
    "                pca = PCA(n_components = j)\n",
    "                pca.fit(currentBunch)\n",
    "                transformed = pca.transform(currentBunch)\n",
    "                print(currentBunch.shape) if DEBUG else None\n",
    "                clusteringMethod.fit(transformed)\n",
    "                unique, counts = np.unique(clusteringMethod.labels_, return_counts=True)\n",
    "                mapping = dict(zip(unique, counts))\n",
    "                outliers = 0;\n",
    "                if -1 in mapping:\n",
    "                    outliers = mapping[-1]\n",
    "                uniqueClusters = len(unique) + outliers\n",
    "                clusterList.append(((i+1)/10,j,np.sum(pca.explained_variance_ratio_),uniqueClusters,outliers))\n",
    "\n",
    "        clusterDict[keys] = clusterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true,
    "tags": [
     "data_display"
    ]
   },
   "outputs": [],
   "source": [
    "print(intensityLabels) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "if EXPLORE:\n",
    "    print(clusterDict.keys())\n",
    "    data = clusterDict['21 5 3']\n",
    "    df = pd.DataFrame(data, columns = ['eps', 'numcomponents', 'variance ratio','num clusters','outliers'])\n",
    "    print(df) if DEBUG else None\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(df['eps'], df['numcomponents'], df['num clusters'], c='skyblue', s=15)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true,
    "tags": [
     "file_writing"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    if os.path.exists('pathClusters'):\n",
    "        shutil.rmtree('pathClusters')\n",
    "    os.mkdir(\"pathClusters\")\n",
    "    pathFiles = {}\n",
    "    for currentKey in intensityLabels:#currently uses the intensityLabels instead of the standard labels from clustering\n",
    "        for index, cluster in enumerate(intensityLabels[currentKey]):\n",
    "            clusterRoot = currentKey.split(\" \")\n",
    "            clusterRoot = \",\".join(clusterRoot)\n",
    "            filename = \"\".join((\"pathClusters/\",clusterRoot,\";\",str(cluster),\".txt\"))#format the filename based on the root node\n",
    "            if filename in pathFiles:\n",
    "                workingFile = pathFiles[filename]\n",
    "            else:\n",
    "                workingFile =  open(filename, \"w+\")\n",
    "                pathFiles[filename] = workingFile\n",
    "            print(str(flatennedPaths[currentKey][cluster])) if DEBUG else None\n",
    "            flatennedPaths[currentKey][index].astype(np.int)\n",
    "            line = str(flatennedPaths[currentKey][index])[1:-1] #flattened paths contain the unedited paths hashed by the root node\n",
    "            line = re.sub(\"'|\\n|[|]\", \"\", line) #get rid of junk characters when converting from an array to a string\n",
    "            workingFile.write(\" \".join((line,\"\\n\")))\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true,
    "tags": [
     "file_writing"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:    \n",
    "    allBunchedPathFiles = open(\"BunchedPathFiles.txt\",\"w+\")\n",
    "    allBunchedPathFiles.write(\"bv_loadDataImage      SWI_Images_resize10.nii \\n\")\n",
    "    allBunchedPathFiles.write(\"bv_loadSegmentedImage csf_seg_8bit_resize10_8bit.nii \\n\")\n",
    "    for fileKeys in pathFiles:\n",
    "        allBunchedPathFiles.write(\"    \".join((\"bv_readPathsFromFile\",pathFiles[fileKeys].name)))\n",
    "        allBunchedPathFiles.write(\"\\n\")\n",
    "        pathFiles[fileKeys].close()\n",
    "    #allPathFiles.write(str(clusteringMethod))\n",
    "    allBunchedPathFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true,
    "tags": [
     "file_writing"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    if os.path.exists('intesnsityClusters'):\n",
    "        shutil.rmtree('intesnsityClusters')\n",
    "    os.mkdir(\"intesnsityClusters\")\n",
    "    intensityFiles = {}\n",
    "    for currentKey in intensityLabels:#currently uses the intensityLabels instead of the standard labels from clustering\n",
    "        for index, cluster in enumerate(intensityLabels[currentKey]):\n",
    "            clusterRoot = currentKey.split(\" \")\n",
    "            clusterRoot = \",\".join(clusterRoot)\n",
    "            filename = \"\".join((\"intesnsityClusters/\",clusterRoot,\";\",str(cluster),\".txt\"))#format the filename based on the root node\n",
    "            if filename in intensityFiles:\n",
    "                workingFile = intensityFiles[filename]\n",
    "            else:\n",
    "                workingFile =  open(filename, \"w+\")\n",
    "                intensityFiles[filename] = workingFile\n",
    "            #print(str(flatennedPaths[currentKey][cluster]))\n",
    "            flatennedPaths[currentKey][index].astype(np.int)\n",
    "            line = str(flatennedPaths[currentKey][index])[1:-1] #flattened paths contain the unedited paths hashed by the root node\n",
    "            line = re.sub(\"'|\\n|[|]\", \"\", line) #get rid of junk characters when converting from an array to a string\n",
    "            workingFile.write(\" \".join((line,\"\\n\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true,
    "tags": [
     "file_writing"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    allBunchedIntensityFiles = open(\"BunchedIntensityFiles.txt\",\"w+\")\n",
    "    allBunchedIntensityFiles.write(\"bv_loadDataImage      SWI_Images_resize10.nii \\n\")\n",
    "    allBunchedIntensityFiles.write(\"bv_loadSegmentedImage csf_seg_8bit_resize10_8bit.nii \\n\")\n",
    "    for fileKeys in intensityFiles:\n",
    "        allBunchedIntensityFiles.write(\"    \".join((\"bv_readPathsFromFile\",intensityFiles[fileKeys].name)))\n",
    "        allBunchedIntensityFiles.write(\"\\n\")\n",
    "        intensityFiles[fileKeys].close()\n",
    "    #allPathFiles.write(str(clusteringMethod))\n",
    "    allBunchedIntensityFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": [
     "file_writing"
    ]
   },
   "outputs": [],
   "source": [
    "if USING_ROOT_BUNCHING:\n",
    "    bunchFile = open(\"bunchedPaths.txt\",\"w+\")\n",
    "    for keys in bunchedPaths:\n",
    "        for line in bunchedPaths[keys]:\n",
    "            for pixelSet in line:\n",
    "                bunchFile.write(re.sub(\"'|\\n|[|]\", \"\", str(pixelSet)))\n",
    "                bunchFile.write(\"\\n\")\n",
    "            bunchFile.write(\"\\n\")\n",
    "    bunchFile.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1575, 59, 1)\n"
     ]
    }
   ],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    allPaths = np.array(allPaths)\n",
    "    allOriginalPaths = np.array(allOriginalPaths)\n",
    "    allIntensityPaths = np.array(allIntensityPaths)\n",
    "    print(allIntensityPaths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    clusteringMethod = DBSCAN(eps = .1, min_samples = 2)\n",
    "    flatAllPaths = np.reshape(allPaths, (allPaths.shape[0], allPaths.shape[1]*allPaths.shape[2])) #flatten\n",
    "    flatAllPaths = normalize(flatAllPaths,norm = 'l2', axis = 0)   \n",
    "    pca = PCA(.95)\n",
    "    pca.fit(flatAllPaths)\n",
    "    transformed = pca.transform(flatAllPaths)\n",
    "    clusteringMethod.fit(transformed)\n",
    "    allPathsLabels = clusteringMethod.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    clusteringMethod = DBSCAN(eps = .1, min_samples = 2)\n",
    "    flatAllOriginalPaths = normalize(allOriginalPaths,norm = 'l2', axis = 0)   \n",
    "    pca = PCA(.95)\n",
    "    pca.fit(flatAllOriginalPaths)\n",
    "    transformed = pca.transform(flatAllOriginalPaths)\n",
    "    clusteringMethod.fit(transformed)\n",
    "    allOriginalPathsLabels = clusteringMethod.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    clusteringMethod = DBSCAN(eps = .1, min_samples = 2)\n",
    "    flatAllIntensityPaths = np.reshape(allIntensityPaths, (allIntensityPaths.shape[0], allIntensityPaths.shape[1]*allIntensityPaths.shape[2])) #flatten\n",
    "    flatAllIntensityPaths = normalize(flatAllIntensityPaths,norm = 'l2', axis = 0)   \n",
    "    pca = PCA(.95)\n",
    "    pca.fit(flatAllIntensityPaths)\n",
    "    transformed = pca.transform(flatAllIntensityPaths)\n",
    "    clusteringMethod.fit(transformed)\n",
    "    allIntensityPathsLabels = clusteringMethod.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    if os.path.exists('allPathClusters'):\n",
    "        shutil.rmtree('allPathClusters')\n",
    "    os.mkdir(\"allPathClusters\")\n",
    "    allPathsDict = {}\n",
    "    for lineNum, label in enumerate(allPathsLabels):\n",
    "        fileName = \"\".join((\"allPathClusters/\",str(label),\".txt\"))\n",
    "        if fileName in allPathsDict:\n",
    "            file = allPathsDict[fileName]\n",
    "        else:\n",
    "            file =  open(fileName, \"w+\")\n",
    "            allPathsDict[fileName] = file\n",
    "        allOriginalPaths[lineNum].astype(np.int)\n",
    "        line = str(allOriginalPaths[lineNum])[1:-1] #flattened paths contain the unedited paths hashed by the root node\n",
    "        line = re.sub(\"'|\\n|[|]\", \"\", line) #get rid of junk characters when converting from an array to a string\n",
    "        file.write(\" \".join((line,\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    allPathFiles = open(\"allPathFiles.txt\",\"w+\")\n",
    "    allPathFiles.write(\"bv_loadDataImage      SWI_Images_resize10.nii \\n\")\n",
    "    allPathFiles.write(\"bv_loadSegmentedImage csf_seg_8bit_resize10_8bit.nii \\n\")\n",
    "    for fileKeys in allPathsDict:\n",
    "        allPathFiles.write(\"    \".join((\"bv_readPathsFromFile\",allPathsDict[fileKeys].name)))\n",
    "        allPathFiles.write(\"\\n\")\n",
    "        allPathsDict[fileKeys].close()\n",
    "    #allPathFiles.write(str(clusteringMethod))\n",
    "    allPathFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    if os.path.exists('allOriginalPathCLusters'):\n",
    "        shutil.rmtree('allOriginalPathCLusters')\n",
    "    os.mkdir(\"allOriginalPathCLusters\")\n",
    "    allOriginalPathsDict = {}\n",
    "    for lineNum, label in enumerate(allOriginalPathsLabels):\n",
    "        fileName = \"\".join((\"allOriginalPathCLusters/\",str(label),\".txt\"))\n",
    "        if fileName in allOriginalPathsDict:\n",
    "            file = allOriginalPathsDict[fileName]\n",
    "        else:\n",
    "            file =  open(fileName, \"w+\")\n",
    "            allOriginalPathsDict[fileName] = file\n",
    "        allOriginalPaths[lineNum].astype(np.int)\n",
    "        line = str(allOriginalPaths[lineNum])[1:-1] #flattened paths contain the unedited paths hashed by the root node\n",
    "        line = re.sub(\"'|\\n|[|]\", \"\", line) #get rid of junk characters when converting from an array to a string\n",
    "        file.write(\" \".join((line,\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    allOriginalPathFiles = open(\"allOriginalPathFiles.txt\",\"w+\")\n",
    "    allOriginalPathFiles.write(\"bv_loadDataImage      SWI_Images_resize10.nii \\n\")\n",
    "    allOriginalPathFiles.write(\"bv_loadSegmentedImage csf_seg_8bit_resize10_8bit.nii \\n\")\n",
    "    for fileKeys in allOriginalPathsDict:\n",
    "        allOriginalPathFiles.write(\"    \".join((\"bv_readPathsFromFile\",allOriginalPathsDict[fileKeys].name)))\n",
    "        allOriginalPathFiles.write(\"\\n\")\n",
    "        allOriginalPathsDict[fileKeys].close()\n",
    "    #allPathFiles.write(str(clusteringMethod))\n",
    "    allOriginalPathFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  0 ...  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    if os.path.exists('allIntensityPathsClusters'):\n",
    "        shutil.rmtree('allIntensityPathsClusters')\n",
    "    os.mkdir(\"allIntensityPathsClusters\")\n",
    "    allIntensitylPathsDict = {}\n",
    "    print(allIntensityPathsLabels)\n",
    "    for lineNum, label in enumerate(allIntensityPathsLabels):\n",
    "        fileName = \"\".join((\"allIntensityPathsClusters/\",str(label),\".txt\"))\n",
    "        if fileName in allIntensitylPathsDict:\n",
    "            file = allIntensitylPathsDict[fileName]\n",
    "        else:\n",
    "            file =  open(fileName, \"w+\")\n",
    "            allIntensitylPathsDict[fileName] = file\n",
    "        allOriginalPaths[lineNum].astype(np.int)\n",
    "        line = str(allOriginalPaths[lineNum])[1:-1] #flattened paths contain the unedited paths hashed by the root node\n",
    "        line = re.sub(\"'|\\n|[|]\", \"\", line) #get rid of junk characters when converting from an array to a string\n",
    "        file.write(\" \".join((line,\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    allIntensityPathFiles = open(\"allIntensityPathFiles.txt\",\"w+\")\n",
    "    allIntensityPathFiles.write(\"bv_loadDataImage      SWI_Images_resize10.nii \\n\")\n",
    "    allIntensityPathFiles.write(\"bv_loadSegmentedImage csf_seg_8bit_resize10_8bit.nii \\n\")\n",
    "    for fileKeys in allIntensitylPathsDict:\n",
    "        allIntensityPathFiles.write(\"    \".join((\"bv_readPathsFromFile\",allIntensitylPathsDict[fileKeys].name)))\n",
    "        allIntensityPathFiles.write(\"\\n\")\n",
    "        allIntensitylPathsDict[fileKeys].close()\n",
    "    #allPathFiles.write(str(clusteringMethod))\n",
    "    allIntensityPathFiles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[278.23419204]\n",
      " [360.61271676]\n",
      " [316.81422925]\n",
      " [ 83.10344828]\n",
      " [424.40677966]\n",
      " [229.16778523]]\n",
      "[0 5 4 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "if not USING_ROOT_BUNCHING:\n",
    "    clusteringMethod = KMeans(n_clusters = 6)\n",
    "    xSlice = np.reshape(allIntensityPaths[:,0], (allIntensityPaths.shape[0], numIntensityParams)) #flatten\n",
    "    #xSlice = normalize(xSlice,norm = 'l1', axis = 0)   \n",
    "    clusteringMethod.fit(xSlice)\n",
    "    xSliceLabels = clusteringMethod.labels_\n",
    "    print(clusteringMethod.cluster_centers_)\n",
    "    print(xSliceLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['79' '0' '0']\n",
      " ['79' '1' '0']\n",
      " ['79' '2' '0']\n",
      " ...\n",
      " ['79' '102' '14']\n",
      " ['79' '103' '14']\n",
      " ['79' '104' '14']]\n",
      "[0 5 4 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(allOriginalPaths[:,0:3])\n",
    "printLabels(\"justXSlice\",xSliceLabels,allOriginalPaths[:,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  0  0 ...  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "printLabels(\"testFolder\",allIntensityPathsLabels,allOriginalPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "a =np.reshape(a,(5,2))\n",
    "print(a) if DEBUG else None\n",
    "#a = np.pad(a,[(0,0),(0,5)], \"constant\", constant_values = 0 )\n",
    "#print(a)\n",
    "a = a/np.max(a,axis=0)\n",
    "print(a) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(swiData[68][8][1]) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "0 5\n",
      "0 6\n",
      "0 7\n",
      "0 8\n"
     ]
    }
   ],
   "source": [
    "for i in range(numIntensityParams):\n",
    "    y = int((i%(2*dilationRadius-1)))\n",
    "    z = int(i/(2*dilationRadius-1))\n",
    "    print(y,z, sep = \" \")\n",
    "print(numIntensityParams) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [[4, 1, 2, 2],\n",
    "...  [1, 3, 9, 3],\n",
    "...  [5, 7, 5, 1]]\n",
    "b = normalize(a, norm='l1', axis=0)#l1 is a average\n",
    "print(b) if DEBUG else None\n",
    "pca = PCA(.9)\n",
    "pca.fit(b)\n",
    "c =pca.transform(b)\n",
    "print(c) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true,
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
    "print(arr) if DEBUG else None\n",
    "arr = np.delete(arr, 0, 1)\n",
    "print(arr) if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
